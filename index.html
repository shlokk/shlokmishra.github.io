<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shlok Mishra</title>
  
  <meta name="author" content="Shlok Mishra ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shlok Mishra</name>
              </p>
              <p>
                I'm a Research Scientist at Meta, where I work on foundational models for video and large language models (LLMs). Prior to this, I did four internships at Google where I worked in Self-Supervised Learning and Generative modelling. 
                I received my PhD from the University of Maryland College Park where I was advised by <a href="http://www.cs.umd.edu/~djacobs/">Prof. David Jacobs.</a>
              </p>

               <p>
              <!-- I primarily work in Self-Supervised Learning and Generative models in Computer Vision. I have done multiple internships at Google. In Summer of 2023, I worked with <a href="https://www.irakemelmacher.com/"> Prof. Ira Kemelmacher-Shlizerman</a> on Try-On diffusion.
              In Summer of 2022, I worked with <a href="https://dilipkay.wordpress.com/">Dilip Krishnan</a>  on combining Self-Supervised learning and Generative modelling. In Summer 2021 I worked with <a href="https://scholar.google.com/citations?user=AliuYd0AAAAJ&hl=en">Christian Haene </a> and <a href="http://www.hossamisack.com/"> Hossam Isack</a>   on Generative models.
              In Summer 2020, I worked with <a href="https://scholar.google.com/citations?user=5aoreLoAAAAJ&hl=en"> Kuntal Sengupta</a>, <a href="https://l2ior.github.io/">Vincent Chu</a>  and <a href="http://sofienbouaziz.com/">Sofien Bouaziz</a>  on Spoof Detection. 
              I have also done 4 student research internships and have been working in Google for last 2.5 years.   -->
              </p>
              <p style="text-align:center">
                <a href="mailto:shlokm@umd.edu">Email</a> &nbsp/&nbsp
                <a href="data/Shlok_Mishra_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=6XJ-4S0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/shlokkkk">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/shlokk/">Github</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/shlokk/">Linkedin</a>
              </p>
              <!-- <p style="text-align:center"><strong>I'm on the Job market for Industry positions.</strong> </p> -->
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Shlok_Mishra.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                </a> </li> 
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2023: 
                </a>Started working as Research Scientist at <a href="https://ai.meta.com/"> Meta-AI. </a> </li>
                </a> </li> 
                </a> </li> 
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2023: 
                </a> Successfully defended my PhD dissertation titled "Self-Supervised Learning on large scale datasets".</li>
                </a> </li> 
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: Invited for Doctoral Consortium at 
                </a> ICCV-2023 and BMVC 2023.</li>
                <!-- </a> </li> 
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2023: Started Internship with <a href="https://www.irakemelmacher.com/"> Prof. Ira Kemelmacher-Shlizerman</a> and Tyler Zhu 
                </a> on Try-On diffusion. </li>   -->
              </a> </li> 
              <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: Three papers accepted to
              </a> to CVPR 2023.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: <a href="https://arxiv.org/abs/2212.00653.pdf">Hyperbolic Contrastive Learning
                </a> is accepted to CVPR 2023.</li> 
             
          <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: <a href="https://arxiv.org/pdf/2211.09117.pdf">MAGE: MAsked Generative Encoder to Unify
            Representation Learning and Image Synthesis
          </a> is accepted to CVPR 2023.</li> 
        </a> </li> 
        <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions
        </a> is accepted to CVPR 2023.</li> 
      </a> </li> 
     
        </a> </li> 
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2022: <a href="https://openreview.net/pdf?id=WXgJN7A69g">Object-aware Cropping for Self-Supervised Learning
                  </a> is accepted to TMLR 12/2022.</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2022: Excited to share our new paper <a href="https://arxiv.org/pdf/2210.16870.pdf">A simple, efficient and scalable contrastive masked autoencoder for learning visual representations.
                    
                  </a> </li> 
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: <a href="https://bmvc2022.mpi-inf.mpg.de/0300.pdf">Learning visual representations for transfer
                    learning by suppressing texture
                  </a> is accepted to BMVC 2022.</li>  
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: Started Internship at Google Research with Dilip Krishnan.</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2021: <a href="https://arxiv.org/pdf/2103.12201.pdf">Improved Presentation Attack Detection Using Image Decomposition
                    </a> is accepted to IJCB 2022 (ORAL).</li>                  
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: Started as a Research Intern at Google Research with Christian Haene.</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2021: <a href="https://proceedings.neurips.cc/paper/2021/file/e5afb0f2dbc6d39b312d7406054cb4c6-Paper.pdf">Robust Contrastive Learning Using Negative Samples
                    with Diminished Semantics</a> is accepted to Neurips 2021</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2021: <a href="https://openaccess.thecvf.com/content/WACV2022/html/Shah_Pose_and_Joint-Aware_Action_Recognition_WACV_2022_paper.html">Pose and Joint-Aware Action Recognition</a> is accepted to WACV 2022</li>
                  <!-- <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2020: Bringing Alive Blurred Moments featured in QS Global Education News</li> -->
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2020: Started as a Research Intern at Google Research with Kuntal Sengupta.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in computer vision, machine learning. Specifically, my current research interests are in Self-Supervised learning, Generative models and building models which can do learn both generative and discriminative features. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hyperbolic.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="95%" valign="middle">
              <a href="https://arxiv.org/abs/2212.00653.pdf">
                <papertitle>Hyperbolic Contrastive Learning for Visual Representations beyond Objects</papertitle>
              </a>
              <br>
              <a href="https://songweige.github.io/">  <strong>Shlok Mishra*</strong> Songwei Ge*</a> , Simon Kornblith, <a href="https://chunliangli.github.io//"> Chun-Liang Li, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs</a> <br>
              <em>CVPR 2023</em>
              <p>We propose a Hyperbolic loss to better encode scene and objects.</p>
              <a href="https://arxiv.org/abs/2212.00653">arXiv</a>
              <a href="https://github.com/shlokk/HCL">code</a>        
              
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/can_paper.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="95%" valign="middle">
              <a href="https://arxiv.org/pdf/2210.16870.pdf">
                <papertitle>A simple, efficient and scalable contrastive masked autoencoder for learning visual representations</papertitle>
              </a>
              <br>
              <strong>Shlok Mishra*</strong>, <a href="https://joshrobinson.mit.edu">Joshua Robinson*</a> , Huiwen Chang </a>, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs </a>, Aaron Sarna, Aaron Maschinot, <a href="https://dilipkay.wordpress.com/">Dilip Krishnan</a> <br>
              <em>preprint</em>
              <p>We propose a simple and scalable contrastive masked autoencoder method.</p>
              <a href="https://arxiv.org/pdf/2210.16870">arXiv</a> 
              <a href="https://github.com/shlokk/mae-contrastive">code</a>
            </td>
          </tr>

          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/halp.png" alt="HaLP" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shah_HaLP_Hallucinating_Latent_Positives_for_Skeleton-Based_Self-Supervised_Learning_of_Actions_CVPR_2023_paper.pdf"> <papertitle>HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions</papertitle></a>  
              
              <br>
              <br>
              Anshul Shah, <a href="https://sites.google.com/site/aniketsealiitkgp/">Aniket Roy*</a>, Ketul Shah*,  <strong>Shlok Mishra</strong>, David Jacobs, <a href="http://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>CVPR 2023</em>
              <p>We hallucinate latent postives for learning skeleton encoders without labels</p>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shah_HaLP_Hallucinating_Latent_Positives_for_Skeleton-Based_Self-Supervised_Learning_of_Actions_CVPR_2023_paper.pdf">paper</a>  
            </td>
          </tr>

          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/texture.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="95%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/file/e5afb0f2dbc6d39b312d7406054cb4c6-Paper.pdf">
                <papertitle>Robust Contrastive Learning Using Negative Samples
                  with Diminished Semantics</papertitle>
              </a>
              <br>
              <a href="https://songweige.github.io/"> Songwei Ge*</a>, <strong>Shlok Mishra</strong>, <a href="https://chunliangli.github.io//"> Chun-Liang Li , <a href="https://chunliangli.github.io//"> Chun-Liang Li, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs</a> <br>
              <em>Neurips 2021</em>
              <p>We show texture based hard negative samples can imporve generalization of contrastive learning method.</p>
              <a href="https://proceedings.neurips.cc/paper/2021/file/e5afb0f2dbc6d39b312d7406054cb4c6-Paper.pdf">paper</a>   
              <a href="https://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives">code</a>     
            </td>
          </tr>

          
          
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/obj_aware.png" alt="ObjAwareCrop" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2112.00319.pdf">
                <papertitle>Object-Aware Cropping for Self-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Shlok Mishra</strong>,  <a href="https://anshulbshah.github.io/">Anshul Shah </a> , <a href="https://ankanbansal.com/">Ankan Bansal</a>, <a href="https://people.cs.umass.edu/~abhyuday/">Abhyuday Jagannatha</a>, Abhishek Sharma, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs</a>, <a href="https://dilipkay.wordpress.com/">Dilip Krishnan</a>
              <br>
              <em>TMLR 12/2022</em>
              <p>A novel cropping strategy for SSL on uncurated datasets</p>
              <a href="https://arxiv.org/pdf/2112.00319.pdf">arXiv</a> 
              <a href="https://github.com/shlokk/object-cropping-ssl">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaser_figure_stylegan.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="95%" valign="middle">    
              <papertitle>Generating Annotated Datasets via Keypoints Conditioned StyleGAN</papertitle>
              </a>
              <br>
              <br>
              <strong>Shlok Mishra</strong>,  <a href="http://www.hossamisack.com/">Hossam Isack</a>   , Sergio Orts-Escolano, Luca Prasso, Rohit Pandey, Franziska Mueller, <a href="https://www.meka.page/"> Abhimitra Meka</a>, Jonathan Taylor,  <a href="https://dilipkay.wordpress.com/">Dilip Krishnan</a> , <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs </a>, Christian Haene.  
              <br>            
              <em>Under Submission</em>
              <p>We use StyleGAN to generate labelled data for keypoint estimation task.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/anisotropic.png" alt="AnisotropicImageNet" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2011.01901.pdf">
                <papertitle>Learning Visual Representations for Transfer Learning by Suppressing Texture</papertitle>
              </a>
              <br>
              <strong>Shlok Mishra</strong>,  <a href="https://anshulbshah.github.io/">Anshul Shah </a> , <a href="https://ankanbansal.com/">Ankan Bansal</a>, <a href="https://ppolon.github.io/">Abhyuday Jagannatha</a>, Jonghyun Choi, <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>, Abhishek Sharma, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs</a>
              <br>
              <em>BMVC 2022</em>
              <p>Anisotropic diffusion based augmentation to reduce texture bias in supervised and self-supervised approaches</p>
              <a href="https://arxiv.org/abs/2011.01901">paper</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spoof_paper.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.12201.pdf">
                <papertitle>Improved Presentation Attack Detection Using Image Decomposition</papertitle>
              </a>
              <br>
              <strong>Shlok Mishra</strong>, Kuntal Sengupta</a>, <a href="https://l2ior.github.io/">Wen-Sheng Chu </a> , Max Horowitz-Gelb,  <a href="http://sofienbouaziz.com/">Sofien Bouaziz </a>, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs </a>
              <em>IJCB 2022 (ORAL)</em>
              <p>We show albedo is a strong cue for spoof detection.</p>
              <a href="https://arxiv.org/pdf/2103.12201">arXiv</a>        
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wacv_crop.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/WACV2022/html/Shah_Pose_and_Joint-Aware_Action_Recognition_WACV_2022_paper.html">
                <papertitle>Pose and Joint-Aware Action Recognition</papertitle>
              </a>
              <br>
               <a href="https://anshulbshah.github.io/">Anshul Shah </a> , <strong>Shlok Mishra</strong>, <a href="https://ankanbansal.com/">Ankan Bansal</a>, <a href="https://www.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>, <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <em>WACV 2022</em>
              <p>We present a new model and loss for Pose-based action recognition</p>
              <a href="https://arxiv.org/abs/2010.08164">arXiv</a> / 
              <a href="https://youtu.be/BqaOlF_LOMA">video</a> /
              <a href="https://github.com/anshulbshah/PoseAction">code</a>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading>
              <p>
                Serving as reviewer for major machine learning and computer vision conferences (CVPR, Neurips, ICML, ICLR, AAAI, ECCV, ICCV, etc) and
                journals (T-PAMI, JMLR, TMLR, TIP , CVIU, IJCV).  In the past I have also served in NLP conferences (ACL, EMNLP).            </p>
            </td>
          </tr>
        </tbody></table>

         


        </tbody></table>

				
      
					
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                Template Credits : <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
