<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shlok Mishra</title>
  
  <meta name="author" content="Shlok Mishra">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Anshul Shah</name>
              </p>
              <p>
              I am a Ph.D. student at University of Maryland in the department of Computer Science. I am advised by <a href="">Prof. David Jacobs</a> working on problems in Computer Vision and Machine Learning. I also obtained an MS in Computer Science from University of Maryland, College Park. 
              </p>

               <p>

              I interned at Microsoft Mixed Reality with Harpreet Sawhney and Ben Lundell in Summer of 2021, 2022 and at Mitsubishi Electric Research Labs with Anoop Cherian in Summer of 2020. I am also actively collaborating with Rachel Reetzke, Ryan Roemmich and Rebecca Landa on early diagnosis of developmental disorders through pose based action recognition.
              </p>
              <p style="text-align:center">
                <a href="mailto:shlokm@umd.edu">Email</a> &nbsp/&nbsp
                <a href="data/AnshulShahCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=akf8VG8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/anshul__shah">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/anshulbshah/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Shlok_Mishra.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2022: Named as an Amazon Fellow as a part of JHU + Amazon initiative for Interactive AI! [<a href="https://ai2ai.engineering.jhu.edu/2022-2023-ai2ai-fellows/">AI2AI</a>,<a href="https://www.amazon.science/latest-news/johns-hopkins-and-amazon-announce-six-fellows-and-nine-faculty-research-awards">Amazon Science</a>] </li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2022: Paper accepted at WACV 2023</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2022: Paper accepted at BMVC 2022 and NeurIPS 2022</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2022: Presented a poster on the colaborative work on ASD at Department of Medicine-Whiting School of Engineering Research Retreat</li>
	          <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: Started 2nd Research Internship with Microsoft's Mixed Reality team</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2022: Selected as <a href="https://iclr.cc/Conferences/2022/Reviewers">Highlighted Reviewer of ICLR 2022</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: Received Student Scholarship for AAAI-2022</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2021: <a href="https://aaai-2022.virtualchair.net/poster_aaai12945">Max-Margin Contrastive Learning</a>  is accepted to AAAI-2022</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: Started as a Research Intern with Microsoft's Mixed Reality team</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2021: <a href="https://openaccess.thecvf.com/content/WACV2022/html/Shah_Pose_and_Joint-Aware_Action_Recognition_WACV_2022_paper.html">Pose and Joint-Aware Action Recognition</a> is accepted to WACV 2022</li>
                  <!-- <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2020: Bringing Alive Blurred Moments featured in QS Global Education News</li> -->
                  <!-- <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2020: Started as a Research Intern at MERL</li> -->
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in computer vision, machine learning. Specifically, my current research interests are in Self-Supervised learning, learning from complex videos and learning from multiple modalities and sensors. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/felmipng.png" alt="FeLMI" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>FeLMi : Few shot Learning with hard Mixup</papertitle>
              <br>
              <a href="https://sites.google.com/site/aniketsealiitkgp/">Aniket Roy</a>, <strong>Anshul Shah</strong>, Ketul Shah, <a href="https://sites.google.com/site/prithvirajdhar274/">Prithviraj Dhar</a>, <a href="http://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>NeurIPS 2022</em>
              <p>We propose hard mixup to improve few shot learning</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmcl_crop.png" alt="MMCL" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11450">
                <papertitle>Max-Margin Contrastive Learning</papertitle>
              </a>
              <br>
              <strong>Anshul Shah</strong>*, <a href="https://optml.mit.edu/">Suvrit Sra</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>, <a href="http://users.cecs.anu.edu.au/~cherian/">Anoop Cherian*</a>
              <br>
              <em>AAAI 2022</em>
              <p>We propose a novel objective for contrastive learning motivated by SVMs</p>
              <a href="https://arxiv.org/abs/2112.11450">arXiv</a> / 
              <a href="https://www.youtube.com/watch?v=MEXxMOP73eE">video</a> /
              <a href="https://github.com/anshulbshah/MMCL">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wacv_crop.png" alt="PoseAction" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/WACV2022/html/Shah_Pose_and_Joint-Aware_Action_Recognition_WACV_2022_paper.html">
                <papertitle>Pose and Joint-Aware Action Recognition</papertitle>
              </a>
              <br>
              <strong>Anshul Shah</strong>, Shlok Mishra, <a href="https://ankanbansal.com/">Ankan Bansal</a>, <a href="https://www.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>, <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
              <em>WACV 2022</em>
              <p>We present a new model and loss for Pose-based action recognition</p>
              <a href="https://arxiv.org/abs/2010.08164">arXiv</a> / 
              <a href="https://youtu.be/BqaOlF_LOMA">video</a> /
              <a href="https://github.com/anshulbshah/PoseAction">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/obj_aware.png" alt="ObjAwareCrop" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2112.00319.pdf">
                <papertitle>Object-Aware Cropping for Self-Supervised Learning</papertitle>
              </a>
              <br>
              Shlok Mishra, <strong>Anshul Shah</strong>, <a href="https://ankanbansal.com/">Ankan Bansal</a>, <a href="https://people.cs.umass.edu/~abhyuday/">Abhyuday Jagannatha</a>, Abhishek Sharma, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs</a>, <a href="https://dilipkay.wordpress.com/">Dilip Krishnan</a>
              <br>
              <em>arXiv 2021</em>
              <p>A novel cropping strategy for SSL on uncurated datasets</p>
              <a href="https://arxiv.org/pdf/2112.00319.pdf">arXiv</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/anisotropic.png" alt="AnisotropicImageNet" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2011.01901.pdf">
                <papertitle>Learning Visual Representations for Transfer Learning by Suppressing Texture</papertitle>
              </a>
              <br>
              Shlok Mishra, <strong>Anshul Shah</strong>, <a href="https://ankanbansal.com/">Ankan Bansal</a>, <a href="https://ppolon.github.io/">Abhyuday Jagannatha</a>, Jonghyun Choi, <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>, Abhishek Sharma, <a href="http://www.cs.umd.edu/~djacobs/">David Jacobs</a>
              <br>
              <em>BMVC 2022</em>
              <p>Anisotropic diffusion based augmentation to reduce texture bias in supervised and self-supervised approaches</p>
              <a href="https://arxiv.org/abs/2011.01901">arXiv</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/blur2video.png" alt="BringingAlive" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Purohit_Bringing_Alive_Blurred_Moments_CVPR_2019_paper.html">
                <papertitle>Bringing Alive Blurred Moments</papertitle>
              </a>
              <br>
              <a href="https://kuldeeppurohit.github.io/">Kuldeep Purohit</a>, <strong>Anshul Shah</strong>, <a href="https://www.ee.iitm.ac.in/~raju/">A.N. Rajagopalan</a>
              <br>
              <em>CVPR 2019 (Oral)</em>
              <p>We present a solution for the goal of extracting a video from a single motion blurred image</p>
              <a href="https://arxiv.org/abs/1804.02913">arXiv</a> 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/reid.png" alt="AttentionVehicle" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/AI_City/Khorramshahi_Attention_Driven_Vehicle_Re-identification_and_Unsupervised_Anomaly_Detection_for_Traffic_CVPRW_2019_paper.html">
                <papertitle>Attention Driven Vehicle Re-identification and Unsupervised Anomaly Detection for Traffic Understanding</papertitle>
              </a>
              <br>
              <a href="https://pirazh.github.io/">Pirazh Khorramshahi</a>, <a href="http://www.neeharperi.com/">Neehar Peri</a>, <a href="https://sites.google.com/view/amitumd/home?authuser=0">Amit Kumar</a>, <strong>Anshul Shah</strong>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>
              <br>
              <em>CVPRW 2019 Nvidia AI City Challenge</em>
              <p>An approach for anomaly detection and vehicle Re-ID on the challenging real-world dataset</p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/blurdetection_crop.png" alt="BlurDetection" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8451765">
                <papertitle>Learning Based Single Image Blur Detection and Segmentation
</papertitle>
              </a>
              <br>
              <a href="https://kuldeeppurohit.github.io/">Kuldeep Purohit</a>, <strong>Anshul Shah</strong>, <a href="https://www.ee.iitm.ac.in/~raju/">A.N. Rajagopalan</a>
              <br>
              <em>ICIP 2018</em>
              <p>A new solution to the problem of obtaining a blur-based segmentation map from a single image affected by motion or defocus blur</p>
            </td>
          </tr>


        </tbody></table>

				
      
					
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                Template Credits : <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
